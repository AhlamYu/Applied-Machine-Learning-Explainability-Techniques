{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LIME for explaining classification models\n",
    "## CHAPTER 04 - *Introduction to LIME for model interpretability*\n",
    "\n",
    "From **Applied Machine Learning Explainability Techniques** by [**Aditya Bhattacharya**](https://www.linkedin.com/in/aditya-bhattacharya-b59155b6/), published by **Packt**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "In this notebook, let us get familiar with the LIME framework for explaining classification models, based on the concepts discussed in Chapter 4 - Introduction to LIME for model interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing the modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the following libraries in Google Colab or your local environment, if not already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pandas numpy matplotlib seaborn scikit-learn lime lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "from lime import submodular_pick\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import LabelEncoder # For transforming categories to integer labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify your configurations as a dict\n",
    "lgb_params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'goss',\n",
    "    'objective': 'binary',\n",
    "    'metric':'binary_logloss',\n",
    "    'metric': {'l2', 'auc'},\n",
    "    'num_leaves': 50,\n",
    "    'learning_rate': 0.1,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'verbose': None,\n",
    "    'num_iteration':100,\n",
    "    'num_threads':7,\n",
    "    'max_depth':12,\n",
    "    'min_data_in_leaf':100,\n",
    "    'alpha':0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the titanic data\n",
    "df_titanic = pd.read_csv('dataset/titanic_train.csv')\n",
    "df_titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation\n",
    "df_titanic.fillna(0,inplace=True)\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "feat = ['PassengerId', 'Pclass_le', 'Sex_le','SibSp_le', 'Parch','Fare']\n",
    "\n",
    "# label encoding textual data\n",
    "df_titanic['Pclass_le'] = le.fit_transform(df_titanic['Pclass'])\n",
    "df_titanic['SibSp_le'] = le.fit_transform(df_titanic['SibSp'])\n",
    "df_titanic['Sex_le'] = le.fit_transform(df_titanic['Sex'])\n",
    "\n",
    "\n",
    "# using train test split to create validation set\n",
    "X_train,X_test,y_train,y_test = train_test_split(df_titanic[feat],df_titanic[['Survived']],test_size=0.3)\n",
    "\n",
    "\n",
    "# def lgb_model(X_train,y_train,X_test,y_test,lgb_params):\n",
    "# create dataset for lightgbm\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test)\n",
    "\n",
    "\n",
    "# training the lightgbm model\n",
    "model = lgb.train(lgb_params,lgb_train,num_boost_round=20,valid_sets=lgb_eval,early_stopping_rounds=5)\n",
    "\n",
    "\n",
    "\n",
    "# this is required as LIME requires class probabilities in case of classification example\n",
    "# LightGBM directly returns probability for class 1 by default \n",
    "\n",
    "def prob(data):\n",
    "    return np.array(list(zip(1-model.predict(data),model.predict(data))))\n",
    "    \n",
    "\n",
    "\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(df_titanic[model.feature_name()].astype(int).values,  \n",
    "mode='classification',training_labels=df_titanic['Survived'],feature_names=model.feature_name())\n",
    "\n",
    "\n",
    "# asking for explanation for LIME model\n",
    "i = 1\n",
    "exp = explainer.explain_instance(df_titanic.loc[i,feat].astype(int).values, prob, num_features=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for SP-LIME\n",
    "import warnings\n",
    "from lime import submodular_pick\n",
    "\n",
    "# Remember to convert the dataframe to matrix values\n",
    "# SP-LIME returns exaplanations on a sample set to provide a non redundant global decision boundary of original model\n",
    "sp_obj = submodular_pick.SubmodularPick(explainer, df_titanic[model.feature_name()].values, \\\n",
    "prob, num_features=5,num_exps_desired=10)\n",
    "\n",
    "[exp.as_pyplot_figure(label=1) for exp in sp_obj.sp_explanations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[exp.show_in_notebook() for exp in sp_obj.sp_explanations]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Breast Cancer Wisconsin (Diagnostic) Data Set - UCI Machine Learning Repository**\n",
    "\n",
    "This dataset is also known as the *breast Cancer* dataset which is used to predict the presence of breast cancer. It is a multivariate dataset used for classification based problems containing 30 different features. More details about this data can be found at - [https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Data-Centric Explainability Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to implement the concepts learnt in the chapter. But I felt that [Deepchecks Open Source Python framework](https://deepchecks.com/) is a fantastic library for implementing most of the concepts related to analyzing Data Consistency and Data Purity. The out-of-the-box API methods from the framework enables us to perform these important steps in minimum lines of code. In this notebook, we will utilize the Deepchecks framework on the Breast Cancer Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's prepare the datasets and the models\n",
    "label = 'target'\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, random_state=123) # Performing an 80-20 split\n",
    "\n",
    "#Creating Deepchecks object\n",
    "train = Dataset(train_df, label=label)\n",
    "test = Dataset(test_df, label=label)\n",
    "\n",
    "# training and testing dataframes\n",
    "x_train = train_df.drop(label, axis = 1)\n",
    "y_train = train_df[label]\n",
    "x_test = test_df.drop(label, axis = 1)\n",
    "y_test = test_df[label]\n",
    "\n",
    "# Model Training\n",
    "model = RandomForestClassifier()\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, we have a very good model with 99% accuracy on the unseen data. We can expect very limited issues related to data purity and consistenyc, but still let us validate using the Deepchecks framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Purity Check with Deepcheck's Single Dataset Integrity Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On the training set\n",
    "purity_check = single_dataset_integrity()\n",
    "purity_check.run(train_dataset = train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On the testing set\n",
    "purity_check.run(test_dataset = test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe from the previous step that using just one line we can perform a thorough analysis of the dataset to observe the presence of missing values, duplicates, label ambiguity or any other common data integrity issues. In case if your problem requires frequent usage of any other masure to evaluate the data integrity, I strongly recommend you to reach out to the owners of the framework with a feature request, or even contribute yourself by raising a pull-request and evolve this unified framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Consistency Check using Deepchecks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use the Train Test Validation Suite to detect the presence of Data Drifts, any Data Distribution issues, Presence of Data Leakage or other data consistency issues between the training and the inference data with minimum lines of code using Deepchecks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_consistency_check = train_test_validation()\n",
    "data_consistency_check.run(model=model, train_dataset=train, test_dataset=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this was a very robust check for data Consistency and the report seems to be interesting. The report summary provides us a glimpse of what we need to focus on and what we dont need to focus on. For this particular dataset, our main concern is related to Dominant Feature's Frequency Change, Single Feature's Contribution thus showing high sensitivity due to these features which are also listed in the report. The difference in Predictive Power Score (PPS) between the train and test data, shows some presence of data leakage. But there is no significant presence of feature drift or concept drift. Further analysis can definitely be done on the issues found, but keeping things simple and easy to understand for all level of readers, I will not recommend over analyzing for this use case. I strongly recommend visiting the deepchecks documentatiosn to learn more at: https://docs.deepchecks.com/.  \n",
    "\n",
    "Next let us review the trust score comparison between the train and test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trust Score Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trust_score_distribution = TrustScoreComparison(min_test_samples = 100)\n",
    "trust_score_distribution.run(train, test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Forecastability using Deepchecks Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_forecastability_check = model_evaluation()\n",
    "data_forecastability_check.run(model=model, train_dataset=train, test_dataset=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the model performance check report from Deepchecks, we can inspect the detailed model performance on various metrics used for the classification problem. Custom metrics can also be used to evaluate the data forecastability. Overall, the dataset is good and well curated as it is evident from the good model accuracy on the test data. But we do see the presence of unused features, which otherwise can be neglected if the model accuracy was not good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let me show a small demo to perform simple data profiling. Although the dataset has multiple features, we will pick up the top 3 features based on feature importance and create data profiles of the training and test set and compare both the profiles to observe presence of any inconsistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the Data Forecastability section, the top three features are as follows:\n",
    "important_features = ['worst radius', 'mean concave points', 'worst concave points']\n",
    "data_profiling_train_df = train_df[important_features]\n",
    "data_profiling_test_df = test_df[important_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_profiling_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_profiling_test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will build a very simple data profile using common statistical measures like mean, median and coeficient of variation. The choice of complexity of statistical measures might vary from dataset to dataset and from use case to use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_profile(df):\n",
    "    '''\n",
    "    Method to build statistical data profiles\n",
    "    '''\n",
    "    profile_parameter = []\n",
    "    profile_value = []\n",
    "    for feature in df.columns:\n",
    "        # Mean\n",
    "        profile_parameter.append('mean_'+ feature)\n",
    "        profile_value.append(np.mean(df[feature]))\n",
    "        # Median\n",
    "        profile_parameter.append('median_'+ feature)\n",
    "        profile_value.append(np.median(df[feature]))\n",
    "        # Coefficient of Variance\n",
    "        profile_parameter.append('cov_'+ feature)\n",
    "        profile_value.append(np.std(df[feature]/np.mean(df[feature])))\n",
    "     \n",
    "    data_profile_df = pd.DataFrame([profile_value], columns = profile_parameter)\n",
    "    return data_profile_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_profile = build_data_profile(data_profiling_train_df)\n",
    "train_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_profile = build_data_profile(data_profiling_test_df)\n",
    "test_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate absolute percentage change in the DataFrame column values\n",
    "pct_df = pd.DataFrame(columns = ['Parameter', '% change'])\n",
    "for col in train_profile.columns:\n",
    "    pct_dict = {'Parameter' : col, '% change' : np.abs((test_profile[col][0] - train_profile[col][0])/train_profile[col][0] * 100)}\n",
    "    pct_df = pct_df.append(pct_dict, ignore_index = True)\n",
    "\n",
    "display(pct_df)                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above table, we can clearly observe that for all statistical measures, the absolute percentage change in the profile values between the train and test set are less than 25%. If the absolute percentage change is more than 20%, this indicates the presence of some data drift in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we have learnt how the deepchecks framework can be effectively used to perform data-centric explanability methods. For certain method, if the framework does not support any built-in out-of-the-box api to implement the concept, we can definitely go with our custom approach similar to what we have seen in the Data Profiling section. Overall, detecting issues related to data consistency like data drifts, data leakage, data purity like missing values, duplicate values, outliers, and data forecasting using model evaluation metrics are certain essential measures that provides valuable explainability to our models and algorithms surrounding the underlying dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. UCI Machine Learning Repository -https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)\n",
    "2. Deepcheck Open Source Python Framework - https://deepchecks.com/\n",
    "3. Some of the utility functions and code are taken from the GitHub Repository of the author - Aditya Bhattacharya https://github.com/adib0073"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
